{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.transforms import Compose\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readInput(subset = 'train'):\n",
    "    aortaDat = pd.read_csv(f'data/aortaP_{subset}_data.csv')\n",
    "    aortaDat.rename({'Unnamed: 0': 'id'}, axis=1, inplace=True)\n",
    "    brachDat = pd.read_csv(f'data/brachP_{subset}_data.csv')\n",
    "    brachDat.rename({'Unnamed: 0': 'id'}, axis=1, inplace=True)\n",
    "\n",
    "    if subset == 'train':\n",
    "        targets = aortaDat[['id', 'target']]\n",
    "        aortaDat = aortaDat.drop(['target'], axis=1)\n",
    "        brachDat = brachDat.drop(['target'], axis=1)\n",
    "    else:\n",
    "        targets = None\n",
    "\n",
    "    aortaDatLong = pd.melt(aortaDat, id_vars=['id'], var_name='time', value_name='aorta')\n",
    "    brachDatLong = pd.melt(brachDat, id_vars=['id'], var_name='time', value_name='brach')\n",
    "\n",
    "    aortaDatLong['time'] = aortaDatLong['time'].str.extract('(\\d+)').astype(int)\n",
    "    brachDatLong['time'] = brachDatLong['time'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "    merge_df = (\n",
    "        brachDatLong\n",
    "            .set_index(['id', 'time'])\n",
    "            .join(aortaDatLong.set_index(['id', 'time']), how='outer', validate='one_to_one')\n",
    "            .reset_index()\n",
    "    )\n",
    "\n",
    "    return (merge_df, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train, all_targets = readInput('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator1 = torch.Generator().manual_seed(42)\n",
    "train_ids, val_ids = random_split(all_train.id.unique(), [0.7, 0.3], generator=generator1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(all_data, subset):\n",
    "    data = []\n",
    "\n",
    "    for idx in subset:\n",
    "        subset_dat = all_data[all_data['id'] == idx].copy()\n",
    "\n",
    "        subset_dat.brach.interpolate(method='linear', inplace=True)\n",
    "        subset_dat.aorta.interpolate(method='linear', inplace=True)\n",
    "\n",
    "        subset_dat.aorta.bfill(inplace=True)\n",
    "        subset_dat.brach.bfill(inplace=True)\n",
    "\n",
    "        subset_dat.aorta.ffill(inplace=True)\n",
    "        subset_dat.brach.ffill(inplace=True)\n",
    "\n",
    "        data.append({\n",
    "            'id': idx,\n",
    "            'data': torch.transpose(torch.tensor(subset_dat[['brach', 'aorta']].values, dtype=torch.float32), 0, 1),\n",
    "            'target': torch.tensor(all_targets[all_targets['id'] == idx]['target'].values[0], dtype=torch.long)\n",
    "        })\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSDataLoader(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.data = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = (self.data[idx]['data'] - 100) / 40\n",
    "\n",
    "        if self.transform:\n",
    "            X = self.transform(X)\n",
    "\n",
    "        if 'target' in self.data[idx]:\n",
    "            return (X, self.data[idx]['target'])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomShift(object):\n",
    "    def __init__(self, shift):\n",
    "        self.shift = shift\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        shift = (torch.rand((1,1)) - 0.5) * 2\n",
    "        return sample + shift\n",
    "    \n",
    "class RandomSubsample(object):\n",
    "    def __init__(self, subsample):\n",
    "        self.subsample = subsample\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        idx = sorted(torch.randperm(sample.shape[1])[:100].numpy())\n",
    "        return sample[:, idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = TSDataLoader(data_load(all_train, train_ids), transform=Compose([RandomShift(0.3), RandomSubsample(75)]))\n",
    "test_data = TSDataLoader(data_load(all_train, val_ids), transform=RandomSubsample(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "torch.device(device)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.LazyConv1d(16, 3),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.LazyBatchNorm1d(),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyConv1d(32, 5),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.LazyBatchNorm1d(),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyConv1d(64, 5),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.LazyBatchNorm1d(),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(64),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajb5d/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(6)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): LazyConv1d(0, 16, kernel_size=(3,), stride=(1,))\n",
       "    (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): LazyBatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "    (4): LazyConv1d(0, 32, kernel_size=(5,), stride=(1,))\n",
       "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): LazyBatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): ReLU()\n",
       "    (8): LazyConv1d(0, 64, kernel_size=(5,), stride=(1,))\n",
       "    (9): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): LazyBatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU()\n",
       "    (12): Flatten(start_dim=1, end_dim=-1)\n",
       "    (13): LazyLinear(in_features=0, out_features=64, bias=True)\n",
       "    (14): ReLU()\n",
       "    (15): LazyLinear(in_features=0, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.822543  [    0/ 2450]\n",
      "loss: 1.778925  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.789219 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.781324  [    0/ 2450]\n",
      "loss: 1.772575  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 23.4%, Avg loss: 1.769860 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.760284  [    0/ 2450]\n",
      "loss: 1.676159  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 22.7%, Avg loss: 1.762182 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.771513  [    0/ 2450]\n",
      "loss: 1.786898  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 21.7%, Avg loss: 1.766655 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.758940  [    0/ 2450]\n",
      "loss: 1.732722  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 25.3%, Avg loss: 1.749240 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.749501  [    0/ 2450]\n",
      "loss: 1.662830  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 25.4%, Avg loss: 1.730559 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.738891  [    0/ 2450]\n",
      "loss: 1.607929  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 1.665658 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.597955  [    0/ 2450]\n",
      "loss: 1.720521  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 30.9%, Avg loss: 1.629869 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.658472  [    0/ 2450]\n",
      "loss: 1.756205  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.684225 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.737359  [    0/ 2450]\n",
      "loss: 1.587423  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.528760 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.687297  [    0/ 2450]\n",
      "loss: 1.609815  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 1.497658 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.454325  [    0/ 2450]\n",
      "loss: 1.496832  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.482296 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.549111  [    0/ 2450]\n",
      "loss: 1.354895  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 1.561980 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.720506  [    0/ 2450]\n",
      "loss: 1.373055  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 23.1%, Avg loss: 2.078307 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.028121  [    0/ 2450]\n",
      "loss: 1.535895  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 23.5%, Avg loss: 1.803860 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.412028  [    0/ 2450]\n",
      "loss: 1.518848  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 34.3%, Avg loss: 1.469514 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.629301  [    0/ 2450]\n",
      "loss: 1.281826  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 28.2%, Avg loss: 1.732470 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.984038  [    0/ 2450]\n",
      "loss: 1.363185  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 36.2%, Avg loss: 1.418860 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.625379  [    0/ 2450]\n",
      "loss: 1.466501  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 32.2%, Avg loss: 1.476508 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.406923  [    0/ 2450]\n",
      "loss: 1.397310  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 35.7%, Avg loss: 1.464190 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.517484  [    0/ 2450]\n",
      "loss: 1.576024  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 36.7%, Avg loss: 1.396297 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.444079  [    0/ 2450]\n",
      "loss: 1.367961  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 37.2%, Avg loss: 1.407329 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.410180  [    0/ 2450]\n",
      "loss: 1.159974  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 35.6%, Avg loss: 1.381969 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.574243  [    0/ 2450]\n",
      "loss: 1.348428  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 37.0%, Avg loss: 1.380312 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.744194  [    0/ 2450]\n",
      "loss: 1.767610  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.513913 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.531232  [    0/ 2450]\n",
      "loss: 1.260687  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 1.386255 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.240811  [    0/ 2450]\n",
      "loss: 1.203021  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.462869 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.457281  [    0/ 2450]\n",
      "loss: 1.405758  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 37.5%, Avg loss: 1.380703 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.541301  [    0/ 2450]\n",
      "loss: 1.754753  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 35.7%, Avg loss: 1.443985 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.542093  [    0/ 2450]\n",
      "loss: 1.277113  [ 1600/ 2450]\n",
      "Test Error: \n",
      " Accuracy: 37.2%, Avg loss: 1.362816 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
